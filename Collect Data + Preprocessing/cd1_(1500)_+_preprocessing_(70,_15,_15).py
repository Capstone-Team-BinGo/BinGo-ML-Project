# -*- coding: utf-8 -*-
"""CD1 (1500) + Preprocessing (70, 15, 15)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-YbPKFyZc33DyETdtaRmct3MJbhogYEI

# **Collect Data**
"""

# LIBRARY
import zipfile
import shutil
import os

os.makedirs("/root/.kaggle", exist_ok=True)
shutil.move("kaggle.json", "/root/.kaggle/kaggle.json")
os.chmod("/root/.kaggle/kaggle.json", 600)

def download_and_extract(dataset_name, zip_filename, extract_to):
    print(f"\nüìÇ Downloading {dataset_name}...")
    os.system(f"kaggle datasets download -d {dataset_name}")
    print(f"üóÅ Extracting {zip_filename}...")
    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:
        zip_ref.extractall(extract_to)

def count_files(folder_path):
    return len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])

def move_and_rename_files(root_dirs, category_map, base_data_dir='data', single_level=False):
    for src_cat, dst_cat in category_map.items():
        target_dir = os.path.join(base_data_dir, dst_cat)
        os.makedirs(target_dir, exist_ok=True)

        for split_name, root_path in root_dirs.items():
            source_dir = os.path.join(root_path, src_cat) if src_cat else root_path
            if not os.path.exists(source_dir):
                continue
            for i, filename in enumerate(os.listdir(source_dir)):
                src_file = os.path.join(source_dir, filename)
                if os.path.isfile(src_file):
                    prefix = split_name if not single_level else dst_cat.replace(" ", "_")
                    new_filename = f"{prefix}_{i}_{filename}" if prefix else f"{i}_{filename}"
                    dst_file = os.path.join(target_dir, new_filename)
                    shutil.move(src_file, dst_file)

        jumlah_file = count_files(target_dir)
        kategori_label = src_cat if src_cat else dst_cat
        print(f"üì¶ Semua file kategori '{kategori_label}' telah dipindahkan ke '{dst_cat}'")
        print(f"üì∏ Jumlah file di '{dst_cat}': {jumlah_file}\n")

def move_files_from_list(source_dirs, target_dir):
    os.makedirs(target_dir, exist_ok=True)
    for src in source_dirs:
        prefix = os.path.basename(src)
        for i, filename in enumerate(os.listdir(src)):
            src_file = os.path.join(src, filename)
            if os.path.isfile(src_file):
                new_filename = f"{prefix}_{i}_{filename}"
                dst_file = os.path.join(target_dir, new_filename)
                shutil.move(src_file, dst_file)
    jumlah_file = count_files(target_dir)
    print(f"üì¶ Semua file dari {len(source_dirs)} folder telah dipindahkan ke '{target_dir}'")
    print(f"üì∏ Jumlah file di '{target_dir}': {jumlah_file}\n")

def cleanup(paths):
    for path in paths:
        if os.path.isfile(path):
            os.remove(path)
        elif os.path.isdir(path):
            shutil.rmtree(path, ignore_errors=True)

# Buat folder data jika belum ada
os.makedirs("data", exist_ok=True)

# Dataset 1: garbage-classification-v2
download_and_extract("sumn2u/garbage-classification-v2", "garbage-classification-v2.zip", "garbage-dataset")

move_and_rename_files({"biological": "garbage-dataset/garbage-dataset"}, {"biological": "Sampah Makanan"}, single_level=True)
move_and_rename_files({"cardboard": "garbage-dataset/garbage-dataset"}, {"cardboard": "Kardus"}, single_level=True)

cleanup(['garbage-dataset', 'garbage-classification-v2.zip'])

# Dataset 2: custom-waste-classification-dataset
download_and_extract("wasifmahmood01/custom-waste-classification-dataset", "custom-waste-classification-dataset.zip", "custom-waste-classification-dataset")

move_and_rename_files(
    {
        'train': 'custom-waste-classification-dataset/wastes/train',
        'test': 'custom-waste-classification-dataset/wastes/test'
    },
    {
        "E-waste": "Elektronik",
        "battery waste": "Baterai",
        "glass waste": "Kaca",
        "light bulbs": "Lampu",
        "metal waste": "Logam",
        "paper waste": "Kertas",
        "plastic waste": "Plastik"
    }
)

cleanup(['custom-waste-classification-dataset', 'custom-waste-classification-dataset.zip'])



# Dataset 4: robin-base
download_and_extract("bahiskaraananda/robin-base", "robin-base.zip", "robin-base")

move_and_rename_files(
    {
        'train': 'robin-base/train',
        'test': 'robin-base/test',
        'val': 'robin-base/val'
    },
    {
        'daun': 'Daun',
        'tekstil': 'Pakaian'
    }
)

cleanup(['robin-base', 'robin-base.zip'])

# Dataset 5: recyclable-and-household-waste-classification
download_and_extract("alistairking/recyclable-and-household-waste-classification", "recyclable-and-household-waste-classification.zip", "recyclable-and-household-waste-classification-dataset")

move_files_from_list([
    "recyclable-and-household-waste-classification-dataset/images/images/styrofoam_food_containers/real_world",
    "recyclable-and-household-waste-classification-dataset/images/images/styrofoam_food_containers/default"
], "data/Sterofom")

cleanup(['recyclable-and-household-waste-classification-dataset', 'recyclable-and-household-waste-classification.zip', 'sample_data'])

# Cek jumlah ganbar perfolder
root_folder = 'data'  # folder utama skema 1
image_exts = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff', '.webp'}

for category in os.listdir(root_folder):
    category_path = os.path.join(root_folder, category)
    if os.path.isdir(category_path):
        count = 0
        for filename in os.listdir(category_path):
            ext = os.path.splitext(filename)[1].lower()
            if ext in image_exts:
                count += 1
        print(f"üìÅ Folder: {category_path} - Jumlah gambar: {count}")

"""# **Pre Processing Data**

## 1. Library
"""

import os
import shutil
import random
import numpy as np
import cv2
from PIL import Image
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, array_to_img, load_img
from sklearn.model_selection import train_test_split
!pip install onnxruntime # Install the 'onnxruntime' package.
from PIL import Image

"""## 2. Load Gambar"""

# Membuat kamus yang menyimpan gambar untuk setiap kelas dalam data
garbage_image = {}

# Tentukan path sumber dataset
path = "/content/data"  # Ganti dengan path ke dataset Anda
path_sub = os.path.join(path)

# Menyimpan gambar dalam kamus berdasarkan kategori
for category in os.listdir(path_sub):
    garbage_image[category] = os.listdir(os.path.join(path_sub, category))

# Menampilkan secara acak 5 gambar di bawah setiap kelas
fig, axs = plt.subplots(len(garbage_image.keys()), 5, figsize=(15, 15))

for i, category_name in enumerate(garbage_image.keys()):
    images = np.random.choice(garbage_image[category_name], 5, replace=False)

    for j, image_name in enumerate(images):
        img_path = os.path.join(path_sub, category_name, image_name)
        img = Image.open(img_path)
        axs[i, j].imshow(img)
        axs[i, j].set(xlabel=category_name, xticks=[], yticks=[])

fig.tight_layout()
plt.show()

"""## 3. Definisi Fungsi Undersampling"""

# Set random.seed = 42
random.seed(42)

# Fungsi undersample in-place (hapus file agar jumlah jadi 1500)
def undersample_inplace(category_dir, target_size=1500):
    images = [f for f in os.listdir(category_dir) if os.path.isfile(os.path.join(category_dir, f))]

    if len(images) <= target_size:
        print(f"Tidak perlu undersample di {category_dir}, jumlah gambar sudah <= {target_size}")
        return

    to_delete = random.sample(images, len(images) - target_size)

    for image in to_delete:
        os.remove(os.path.join(category_dir, image))

    print(f"‚úÖ Undersample selesai untuk {category_dir}. {len(to_delete)} gambar dihapus.")

"""## 4. Definisi Fungsi Resize Gambar"""

def resize_image(input_dir, output_dir, target_size=(224, 224)):
    for image_name in os.listdir(input_dir):
        image_path = os.path.join(input_dir, image_name)
        try:
            with Image.open(image_path) as img:
                if img.mode in ("RGBA", "P"):
                    img = img.convert("RGB")
                img = img.resize(target_size)
                img.save(os.path.join(output_dir, image_name), format="JPEG")
        except Exception as e:
            print(f"‚ùå Gagal resize {image_path}: {e}")

"""## 5. Definisi Fungsi Augmentasi"""

import os
import random
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img

def augment_and_add_images(source_category_dir, target_category_dir, target_size=1500):
    images = os.listdir(source_category_dir)
    existing_count = len(images)
    additional_images_needed = target_size - existing_count

    if additional_images_needed <= 0:
        print(f"‚úÖ {source_category_dir} sudah cukup gambar (>= {target_size})")
        return

    datagen = ImageDataGenerator(
        rotation_range=40,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest'
    )

    os.makedirs(target_category_dir, exist_ok=True)

    i = 0
    # Acak urutan gambar agar variasi lebih baik
    random.shuffle(images)

    while i < additional_images_needed:
        for image in images:
            if i >= additional_images_needed:
                break

            img_path = os.path.join(source_category_dir, image)
            try:
                img = load_img(img_path)

                if img.mode in ("RGBA", "P"):
                    img = img.convert("RGB")

                x = img_to_array(img)
                x = x.reshape((1,) + x.shape)

                # Hanya generate satu augmentasi per gambar per loop
                for batch in datagen.flow(x, batch_size=1):
                    augmented_image = array_to_img(batch[0])

                    if augmented_image.mode in ("RGBA", "P"):
                        augmented_image = augmented_image.convert("RGB")

                    save_path = os.path.join(target_category_dir, f"aug_{i}.jpg")
                    augmented_image.save(save_path, format="JPEG")

                    i += 1
                    break  # Lanjut ke gambar berikutnya

            except Exception as e:
                print(f"‚ùå Gagal augment {img_path}: {e}")

    print(f"‚úÖ Augmentasi selesai. {i} gambar ditambahkan ke {target_category_dir}.")

"""## 6. Definisi Fungsi Splitting Data"""

# Fungsi untuk Pembagian Dataset ke dalam Training, Validation, dan Test
def split_dataset(source_dir, target_dir, categories):
    for category in categories:
        category_dir = os.path.join(source_dir, category)

        images = os.listdir(category_dir)

        # Membagi data menjadi 80% training, 10% validation, 10% test
        train, test = train_test_split(images, test_size=0.3, random_state=42)
        val, test = train_test_split(test, test_size=0.5, random_state=42)

        # Membuat folder untuk training, validation, dan test di target_dir
        for split, data in zip(['train', 'val', 'test'], [train, val, test]):
            split_dir = os.path.join(target_dir, split, category)
            if not os.path.exists(split_dir):
                os.makedirs(split_dir)

            for image in data:
                shutil.copy(os.path.join(category_dir, image), os.path.join(split_dir, image))

    print("Pembagian dataset selesai: Training, Validation, dan Test.")

"""## 7. Main Program

### a. Sumber Directory
"""

source_dir = '/content/data'
target_dir = '/content/data_final'

categories = ['Baterai', 'Plastik', 'Lampu', 'Sterofom', 'Kaca',
              'Pakaian', 'Logam', 'Sampah Makanan', 'Elektronik', 'Kertas',
              'Daun', 'Kardus']

"""### b. Resize, Augmentasi, dan Pembagian Dataset"""

for category in categories:
    source_category_dir = os.path.join(source_dir, category)
    target_category_dir = os.path.join(target_dir, category)

    os.makedirs(target_category_dir, exist_ok=True)

    # Salin gambar ke target folder dengan konversi ke RGB jika perlu
    for image_name in os.listdir(source_category_dir):
        image_path = os.path.join(source_category_dir, image_name)
        target_path = os.path.join(target_category_dir, image_name)

        try:
            with Image.open(image_path) as img:
                if img.mode in ("RGBA", "P"):
                    img = img.convert("RGB")
                    img.save(image_path, format="JPEG")  # overwrite as JPEG
        except Exception as e:
            print(f"‚ùå Gagal konversi {image_path}: {e}")

        shutil.copy(image_path, target_path)

    # Resize semua gambar
    resize_image(target_category_dir, target_category_dir, target_size=(224, 224))

    # Undersample atau augment sesuai jumlah gambar
    image_count = len(os.listdir(target_category_dir))
    if image_count > 1500:
        undersample_inplace(target_category_dir, target_size=1500)
    else:
        augment_and_add_images(target_category_dir, target_category_dir, target_size=1500)

for category in os.listdir(target_dir):
    category_path = os.path.join(target_dir, category)
    if os.path.isdir(category_path):
        count = len([f for f in os.listdir(category_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])
        print(f"üìÅ {category}: {count} gambar")

# Membuat kamus yang menyimpan gambar untuk setiap kelas dalam data
garbage_image_final = {}

# Tentukan path sumber dataset
path = "/content/data_final"
path_sub = os.path.join(path)

# Menyimpan gambar dalam kamus berdasarkan kategori
for category in os.listdir(path_sub):
    garbage_image_final[category] = os.listdir(os.path.join(path_sub, category))

# Menampilkan secara acak 5 gambar di bawah setiap kelas
fig, axs = plt.subplots(len(garbage_image_final.keys()), 5, figsize=(15, 15))

for i, category_name in enumerate(garbage_image_final.keys()):
    images = np.random.choice(garbage_image_final[category_name], 5, replace=False)

    for j, image_name in enumerate(images):
        img_path = os.path.join(path_sub, category_name, image_name)
        img = Image.open(img_path)
        axs[i, j].imshow(img)
        axs[i, j].set(xlabel=category_name, xticks=[], yticks=[])

fig.tight_layout()
plt.show()

"""### c. Pembagian Dataset"""

data_final_split = '/content/data_final_split'
# Membuat folder jika belum ada
os.makedirs(data_final_split, exist_ok=True)

# Pembagian dataset menjadi training, validation, dan test
split_dataset(source_dir=target_dir, target_dir=data_final_split, categories=categories)

for split in ['train', 'val', 'test']:
    for cat in categories:
        folder = os.path.join(target_dir, split, cat)
        count = len(os.listdir(folder))
        print(f"{split}/{cat}: {count} gambar")

