# -*- coding: utf-8 -*-
"""Final - Machine Learning - Capstone

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14eYLonfb6iS9GwUXP1WvCVIfC0MJtQfr

# **Collect Data**

## 1. Import Libraries
"""

!pip install gdown
import gdown
import zipfile
import shutil
import os

# ID file kaggle.json dari link Google Drive
file_id = "11y9rX8FPN-fBWw0q3nzJdUGuq4ugXHxc"
gdown.download(f"https://drive.google.com/uc?id={file_id}", "kaggle.json", quiet=False)

os.makedirs("/root/.kaggle", exist_ok=True)
shutil.copy("kaggle.json", "/root/.kaggle/kaggle.json")
os.chmod("/root/.kaggle/kaggle.json", 600)

"""## 2. Definisi Fungsi untuk *Load Data* dari Kaggle"""

def download_and_extract(dataset_name, zip_filename, extract_to):
    print(f"\nüìÇ Downloading {dataset_name}...")
    os.system(f"kaggle datasets download -d {dataset_name}")
    print(f"üóÅ Extracting {zip_filename}...")
    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:
        zip_ref.extractall(extract_to)

def count_files(folder_path):
    return len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])

def move_and_rename_files(root_dirs, category_map, base_data_dir='data', single_level=False):
    for src_cat, dst_cat in category_map.items():
        target_dir = os.path.join(base_data_dir, dst_cat)
        os.makedirs(target_dir, exist_ok=True)

        for split_name, root_path in root_dirs.items():
            source_dir = os.path.join(root_path, src_cat) if src_cat else root_path
            if not os.path.exists(source_dir):
                continue
            for i, filename in enumerate(os.listdir(source_dir)):
                src_file = os.path.join(source_dir, filename)
                if os.path.isfile(src_file):
                    prefix = split_name if not single_level else dst_cat.replace(" ", "_")
                    new_filename = f"{prefix}_{i}_{filename}" if prefix else f"{i}_{filename}"
                    dst_file = os.path.join(target_dir, new_filename)
                    shutil.move(src_file, dst_file)

        jumlah_file = count_files(target_dir)
        kategori_label = src_cat if src_cat else dst_cat
        print(f"üì¶ Semua file kategori '{kategori_label}' telah dipindahkan ke '{dst_cat}'")
        print(f"üì∏ Jumlah file di '{dst_cat}': {jumlah_file}\n")

def move_files_from_list(source_dirs, target_dir):
    os.makedirs(target_dir, exist_ok=True)
    for src in source_dirs:
        prefix = os.path.basename(src)
        for i, filename in enumerate(os.listdir(src)):
            src_file = os.path.join(src, filename)
            if os.path.isfile(src_file):
                new_filename = f"{prefix}_{i}_{filename}"
                dst_file = os.path.join(target_dir, new_filename)
                shutil.move(src_file, dst_file)
    jumlah_file = count_files(target_dir)
    print(f"üì¶ Semua file dari {len(source_dirs)} folder telah dipindahkan ke '{target_dir}'")
    print(f"üì∏ Jumlah file di '{target_dir}': {jumlah_file}\n")

def cleanup(paths):
    for path in paths:
        if os.path.isfile(path):
            os.remove(path)
        elif os.path.isdir(path):
            shutil.rmtree(path, ignore_errors=True)

# Buat folder data
os.makedirs("data", exist_ok=True)

"""## 3. *Load Dataset*

### Dataset 1: garbage-classification-v2
"""

download_and_extract("sumn2u/garbage-classification-v2", "garbage-classification-v2.zip", "garbage-dataset")

move_and_rename_files({"biological": "garbage-dataset/garbage-dataset"}, {"biological": "Sampah Makanan"}, single_level=True)
move_and_rename_files({"cardboard": "garbage-dataset/garbage-dataset"}, {"cardboard": "Kardus"}, single_level=True)

cleanup(['garbage-dataset', 'garbage-classification-v2.zip'])

"""### Dataset 2: custom-waste-classification-dataset"""

download_and_extract("wasifmahmood01/custom-waste-classification-dataset", "custom-waste-classification-dataset.zip", "custom-waste-classification-dataset")

move_and_rename_files(
    {
        'train': 'custom-waste-classification-dataset/wastes/train',
        'test': 'custom-waste-classification-dataset/wastes/test'
    },
    {
        "E-waste": "Elektronik",
        "battery waste": "Baterai",
        "glass waste": "Kaca",
        "light bulbs": "Lampu",
        "metal waste": "Logam",
        "paper waste": "Kertas",
        "plastic waste": "Plastik"
    }
)

cleanup(['custom-waste-classification-dataset', 'custom-waste-classification-dataset.zip'])

"""### Dataset 3: robin-base"""

download_and_extract("bahiskaraananda/robin-base", "robin-base.zip", "robin-base")

move_and_rename_files(
    {
        'train': 'robin-base/train',
        'test': 'robin-base/test',
        'val': 'robin-base/val'
    },
    {
        'daun': 'Daun',
        'tekstil': 'Pakaian'
    }
)

cleanup(['robin-base', 'robin-base.zip'])

"""### Dataset 4: recyclable-and-household-waste-classification"""

download_and_extract("alistairking/recyclable-and-household-waste-classification", "recyclable-and-household-waste-classification.zip", "recyclable-and-household-waste-classification-dataset")

move_files_from_list([
    "recyclable-and-household-waste-classification-dataset/images/images/styrofoam_food_containers/real_world",
    "recyclable-and-household-waste-classification-dataset/images/images/styrofoam_food_containers/default"
], "data/Sterofom")

cleanup(['recyclable-and-household-waste-classification-dataset', 'recyclable-and-household-waste-classification.zip', 'sample_data'])

"""## 4. Cek Jumlah Gambar per Folder"""

root_folder = 'data'
image_exts = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff', '.webp'}

for category in os.listdir(root_folder):
    category_path = os.path.join(root_folder, category)
    if os.path.isdir(category_path):
        count = 0
        for filename in os.listdir(category_path):
            ext = os.path.splitext(filename)[1].lower()
            if ext in image_exts:
                count += 1
        print(f"üìÅ Folder: {category_path} - Jumlah gambar: {count}")

"""# **Pre Processing Data**

## 1. Import Libraries
"""

import os
import shutil
import random
import numpy as np
import cv2
from PIL import Image
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, array_to_img, load_img
from sklearn.model_selection import train_test_split
!pip install onnxruntime
from PIL import Image

"""## 2. Menampilkan Preview Gambar"""

# Membuat kamus yang menyimpan gambar untuk setiap kelas dalam data
garbage_image = {}

# Tentukan path sumber dataset
path = "/content/data"  # Ganti dengan path ke dataset Anda
path_sub = os.path.join(path)

# Menyimpan gambar dalam kamus berdasarkan kategori
for category in os.listdir(path_sub):
    garbage_image[category] = os.listdir(os.path.join(path_sub, category))

# Menampilkan secara acak 5 gambar di bawah setiap kelas
fig, axs = plt.subplots(len(garbage_image.keys()), 5, figsize=(15, 15))

for i, category_name in enumerate(garbage_image.keys()):
    images = np.random.choice(garbage_image[category_name], 5, replace=False)

    for j, image_name in enumerate(images):
        img_path = os.path.join(path_sub, category_name, image_name)
        img = Image.open(img_path)
        axs[i, j].imshow(img)
        axs[i, j].set(xlabel=category_name, xticks=[], yticks=[])

fig.tight_layout()
plt.show()

"""## 3. Definisi Fungsi

### Definisi Fungsi Undersampling
"""

# Set random.seed = 42
random.seed(42)

# Fungsi undersample in-place (hapus file agar jumlah jadi 1500)
def undersample_inplace(category_dir, target_size=1500):
    images = [f for f in os.listdir(category_dir) if os.path.isfile(os.path.join(category_dir, f))]

    if len(images) <= target_size:
        print(f"Tidak perlu undersample di {category_dir}, jumlah gambar sudah <= {target_size}")
        return

    to_delete = random.sample(images, len(images) - target_size)

    for image in to_delete:
        os.remove(os.path.join(category_dir, image))

    print(f"‚úÖ Undersample selesai untuk {category_dir}. {len(to_delete)} gambar dihapus.")

"""### Definisi Fungsi Resize Gambar"""

def resize_image(input_dir, output_dir, target_size=(224, 224)):
    for image_name in os.listdir(input_dir):
        image_path = os.path.join(input_dir, image_name)
        try:
            with Image.open(image_path) as img:
                if img.mode in ("RGBA", "P"):
                    img = img.convert("RGB")
                img = img.resize(target_size)
                img.save(os.path.join(output_dir, image_name), format="JPEG")
        except Exception as e:
            print(f"‚ùå Gagal resize {image_path}: {e}")

"""### Definisi Fungsi Augmentasi"""

def augment_and_add_images(source_category_dir, target_category_dir, target_size=1500):
    images = os.listdir(source_category_dir)
    existing_count = len(images)
    additional_images_needed = target_size - existing_count

    if additional_images_needed <= 0:
        print(f"‚úÖ {source_category_dir} sudah cukup gambar (>= {target_size})")
        return

    datagen = ImageDataGenerator(
        rotation_range=40,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest'
    )

    os.makedirs(target_category_dir, exist_ok=True)

    i = 0
    # Acak urutan gambar agar variasi lebih baik
    random.shuffle(images)

    while i < additional_images_needed:
        for image in images:
            if i >= additional_images_needed:
                break

            img_path = os.path.join(source_category_dir, image)
            try:
                img = load_img(img_path)

                if img.mode in ("RGBA", "P"):
                    img = img.convert("RGB")

                x = img_to_array(img)
                x = x.reshape((1,) + x.shape)

                # Hanya generate satu augmentasi per gambar per loop
                for batch in datagen.flow(x, batch_size=1):
                    augmented_image = array_to_img(batch[0])

                    if augmented_image.mode in ("RGBA", "P"):
                        augmented_image = augmented_image.convert("RGB")

                    save_path = os.path.join(target_category_dir, f"aug_{i}.jpg")
                    augmented_image.save(save_path, format="JPEG")

                    i += 1
                    break  # Lanjut ke gambar berikutnya

            except Exception as e:
                print(f"‚ùå Gagal augment {img_path}: {e}")

    print(f"‚úÖ Augmentasi selesai. {i} gambar ditambahkan ke {target_category_dir}.")

"""### Definisi Fungsi Splitting Data"""

# Fungsi untuk Pembagian Dataset ke dalam Training, Validation, dan Test
def split_dataset(source_dir, target_dir, categories):
    for category in categories:
        category_dir = os.path.join(source_dir, category)

        images = os.listdir(category_dir)

        # Membagi data menjadi 70% training, 15% validation, 15% test
        train, test = train_test_split(images, test_size=0.3, random_state=42)
        val, test = train_test_split(test, test_size=0.5, random_state=42)

        # Membuat folder untuk training, validation, dan test di target_dir
        for split, data in zip(['train', 'val', 'test'], [train, val, test]):
            split_dir = os.path.join(target_dir, split, category)
            if not os.path.exists(split_dir):
                os.makedirs(split_dir)

            for image in data:
                shutil.copy(os.path.join(category_dir, image), os.path.join(split_dir, image))

    print("Pembagian dataset selesai: Training, Validation, dan Test.")

"""## 4. *Main Program*

### Sumber Directory
"""

source_dir = '/content/data'
target_dir = '/content/data_final'

categories = ['Baterai', 'Plastik', 'Lampu', 'Sterofom', 'Kaca',
              'Pakaian', 'Logam', 'Sampah Makanan', 'Elektronik', 'Kertas',
              'Daun', 'Kardus']

"""### Resize dan Augmentasi"""

for category in categories:
    source_category_dir = os.path.join(source_dir, category)
    target_category_dir = os.path.join(target_dir, category)

    os.makedirs(target_category_dir, exist_ok=True)

    # Salin gambar ke target folder dengan konversi ke RGB
    for image_name in os.listdir(source_category_dir):
        image_path = os.path.join(source_category_dir, image_name)
        target_path = os.path.join(target_category_dir, image_name)

        try:
            with Image.open(image_path) as img:
                if img.mode in ("RGBA", "P"):
                    img = img.convert("RGB")
                    img.save(image_path, format="JPEG")
        except Exception as e:
            print(f"‚ùå Gagal konversi {image_path}: {e}")

        shutil.copy(image_path, target_path)

    # Resize semua gambar
    resize_image(target_category_dir, target_category_dir, target_size=(224, 224))

    # Undersample atau augment sesuai jumlah gambar
    image_count = len(os.listdir(target_category_dir))
    if image_count > 1500:
        undersample_inplace(target_category_dir, target_size=1500)
    else:
        augment_and_add_images(target_category_dir, target_category_dir, target_size=1500)

"""## 5. Mengecek Jumlah Gambar Tiap Kategori"""

for category in os.listdir(target_dir):
    category_path = os.path.join(target_dir, category)
    if os.path.isdir(category_path):
        count = len([f for f in os.listdir(category_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])
        print(f"üìÅ {category}: {count} gambar")

"""## 6. Menampilkan Preview Gambar Setelah di *Preprocessing*"""

# Membuat kamus yang menyimpan gambar untuk setiap kelas dalam data
garbage_image_final = {}

# Tentukan path sumber dataset
path = "/content/data_final"
path_sub = os.path.join(path)

# Menyimpan gambar dalam kamus berdasarkan kategori
for category in os.listdir(path_sub):
    garbage_image_final[category] = os.listdir(os.path.join(path_sub, category))

# Menampilkan secara acak 5 gambar di bawah setiap kelas
fig, axs = plt.subplots(len(garbage_image_final.keys()), 5, figsize=(15, 15))

for i, category_name in enumerate(garbage_image_final.keys()):
    images = np.random.choice(garbage_image_final[category_name], 5, replace=False)

    for j, image_name in enumerate(images):
        img_path = os.path.join(path_sub, category_name, image_name)
        img = Image.open(img_path)
        axs[i, j].imshow(img)
        axs[i, j].set(xlabel=category_name, xticks=[], yticks=[])

fig.tight_layout()
plt.show()

"""## Pembagian Dataset"""

data_final_split = '/content/data_final_split'

# Membuat folder data_final_split
os.makedirs(data_final_split, exist_ok=True)

# Pembagian dataset menjadi training, validation, dan test
split_dataset(source_dir=target_dir, target_dir=data_final_split, categories=categories)

for split in ['train', 'val', 'test']:
    for cat in categories:
        folder = os.path.join(data_final_split, split, cat)
        count = len(os.listdir(folder))
        print(f"{split}/{cat}: {count} gambar")

"""# **Modeling**

## 1. Import Libraries
"""

# Built-in dan library dasar
import os
import random
import numpy as np
import matplotlib.pyplot as plt

# TensorFlow dan Keras
import tensorflow as tf
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.applications.efficientnet import preprocess_input
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

# Scikit-learn untuk evaluasi
from sklearn.metrics import classification_report, confusion_matrix

# Seaborn untuk heatmap
import seaborn as sns

"""## 2. Mengambil path dataset"""

train_dir = '/content/data_final_split/train'
val_dir = '/content/data_final_split/val'
test_dir = '/content/data_final_split/test'

"""## 3. Menerapkan set seed agar hasil konsisten"""

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
set_seed(42)

"""## 4. Data generators"""

train_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input,
    rotation_range=30,
    width_shift_range=0.3,
    height_shift_range=0.3,
    shear_range=0.3,
    zoom_range=0.3,
    horizontal_flip=True,
    brightness_range=[0.5, 1.5],
    channel_shift_range=20.0,
    fill_mode='nearest'
)

val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)

"""## 5. Cek jumlah kelas dari generator"""

train_gen = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224,224),
    batch_size=32,
    class_mode='categorical',
    shuffle=True,
    seed=42
)

num_classes = train_gen.num_classes # Mendapatkan informasi jumlah kelas yang ada

val_gen = val_datagen.flow_from_directory(
    val_dir,
    target_size=(224,224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False,
    seed=42
)

test_gen = test_datagen.flow_from_directory(
    test_dir,
    target_size=(224,224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False,
    seed=42
)

# Mengetahui banyaknya kelas dan kode pada setiap kelas
print(f"Jumlah kelas yang terdeteksi oleh generator: {num_classes}")
print(f"Mapping kelas ke indeks: {train_gen.class_indices}")

"""## 6. Membangun model EfficientNetB0 dengan modifikasi Conv2D dan MaxPooling2D + Training"""

# Load base model
base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = False

# Membangun mode
inputs = Input(shape=(224, 224, 3))
x = base_model(inputs, training=False)

# Menambahkan Conv2D + MaxPooling setelah pretrained model
x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
x = MaxPooling2D(pool_size=(2, 2))(x)

x = GlobalAveragePooling2D()(x)
x = Dropout(0.3)(x)
outputs = Dense(num_classes, activation='softmax')(x)

model = Model(inputs, outputs)

# Compile awal (head training)
model.compile(
    optimizer=Adam(learning_rate=1e-3),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Callback
early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=4, verbose=1)
checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

callbacks = [early_stop, reduce_lr, checkpoint]

# Train head saja
history1 = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=10,
    callbacks=callbacks
)

# Fine-tuning EfficientNet
base_model.trainable = True
for layer in base_model.layers[:50]:
    layer.trainable = False

# Compile lagi dengan learning rate kecil
model.compile(
    optimizer=Adam(learning_rate=1e-5),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Melanjutkan training
history2 = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=50,
    callbacks=callbacks
)

# Menggabungkan hasil training
def combine_history(h1, h2):
    history = {}
    for key in h1.history:
        history[key] = h1.history[key] + h2.history[key]
    return history

full_history = combine_history(history1, history2)

"""## 7. Load bobot terbaik"""

model.load_weights('best_model.h5')

"""## 8. Visualisasi plot akurasi dan loss training"""

# Plot hasil training
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.plot(full_history['accuracy'], label='Train Acc')
plt.plot(full_history['val_accuracy'], label='Val Acc')
plt.title('Accuracy per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(full_history['loss'], label='Train Loss')
plt.plot(full_history['val_loss'], label='Val Loss')
plt.title('Loss per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

"""## 9. Evaluasi pada data test"""

test_loss, test_acc = model.evaluate(test_gen)
print(f"Test Accuracy: {test_acc:.4f}")
print(f"Test Loss: {test_loss:.4f}")

"""## 10. Prediksi pada setiap kelas"""

pred_probs = model.predict(test_gen, verbose=1)
y_pred = np.argmax(pred_probs, axis=1)
y_true = test_gen.classes
class_names = list(test_gen.class_indices.keys())

"""## 11. Classification Report"""

print("Classification Report:")
print(classification_report(y_true, y_pred, target_names=class_names))

"""## 12. Confusion Matrix"""

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names,
            yticklabels=class_names)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.xticks(rotation=45)
plt.yticks(rotation=45)
plt.tight_layout()
plt.show()

"""# **Inference**

## 1. Import Libraries
"""

from google.colab import files
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.efficientnet import preprocess_input
import numpy as np
import os
from tensorflow.keras.models import load_model
from PIL import Image
import matplotlib.pyplot as plt

!pip install gdown
import gdown

"""## 2. Load model hasil training"""

model = load_model('best_model.h5')

"""## 3. Mengelompokkan Kelas ke dalam Grup"""

class_names = [
    'Baterai', 'Daun', 'Elektronik', 'Kaca', 'Kardus',
    'Kertas', 'Lampu', 'Logam', 'Pakaian',
    'Plastik', 'Sampah Makanan', 'Sterofom'
]

group_map = {
    'Logam': 'Anorganik',
    'Plastik': 'Anorganik',
    'Pakaian': 'Anorganik',
    'Kaca': 'Anorganik',
    'Sterofom': 'Anorganik',
    'Daun': 'Organik',
    'Kardus': 'Organik',
    'Sampah Makanan': 'Organik',
    'Kertas': 'Organik',
    'Baterai': 'B3',
    'Lampu': 'B3',
    'Elektronik': 'B3'
}

"""## 4. Definisi Fungsi Load dan Prediksi Gambar"""

def prepare_image(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_array = preprocess_input(img_array)
    img_array = np.expand_dims(img_array, axis=0)
    return img_array

def predict_image(img_path):
    img_ready = prepare_image(img_path)
    preds = model.predict(img_ready)
    pred_idx = np.argmax(preds, axis=1)[0]
    pred_class = class_names[pred_idx]
    pred_group = group_map.get(pred_class, 'Unknown')
    return pred_class, pred_group

"""## 5. Upload Gambar untuk prediksi"""

uploaded = files.upload()

for filename in uploaded.keys():
    # Preview gambar
    img = Image.open(filename)
    plt.imshow(img)
    plt.axis('off')
    plt.show()

    # Prediksi
    pred_class, pred_group = predict_image(filename)
    print(f"{filename} -> Sampah: {pred_class}, Kategori: {pred_group}")