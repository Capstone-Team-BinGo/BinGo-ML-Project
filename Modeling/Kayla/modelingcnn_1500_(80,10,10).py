# -*- coding: utf-8 -*-
"""ModelingCNN_1500_(80,10,10)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_P8n92rmJn7vqACcjDGkvHx5JsFA3xnk
"""

!pip install gdown
import gdown

file_id = '1AEenvsEPa35bHgnqGpcXK8hqbp1iV6Ya'
gdown.download(f'https://drive.google.com/uc?id={file_id}', 'dataset_final_split.zip', quiet=False)

import os
import zipfile

with zipfile.ZipFile('dataset_final_split.zip', 'r') as zip_ref:
    zip_ref.extractall('dataset')

import shutil

# Pindahkan folder
shutil.move('dataset/content/data_final_split', 'data_final_split')

# Hapus folder 'dataset/content'
shutil.rmtree('dataset')

base_dir = 'data_final_split'
splits = ['train', 'val', 'test']

for split in splits:
    print(f"\n=== {split.upper()} ===")
    split_dir = os.path.join(base_dir, split)
    for class_name in os.listdir(split_dir):
        class_dir = os.path.join(split_dir, class_name)
        if os.path.isdir(class_dir):
            n_files = len(os.listdir(class_dir))
            print(f"{class_name}: {n_files} gambar")

"""# MODELING"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Augmentasi data untuk training
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    zoom_range=0.2,
    horizontal_flip=True
)

# Hanya rescale untuk validasi
val_datagen = ImageDataGenerator(rescale=1./255)

# Buat data generator
train_generator = train_datagen.flow_from_directory(
    directory=f'{base_dir}/train',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)

val_generator = val_datagen.flow_from_directory(
    directory=f'{base_dir}/val',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)

print(train_generator.class_indices)

"""### MODEL 1"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Model CNN sederhana
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(train_generator.num_classes, activation='softmax')
])

# Kompilasi model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Callback
callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)
]

# Training model
history = model.fit(
    train_generator,
    epochs=50,
    validation_data=val_generator,
    callbacks=callbacks
)

# Evaluasi pakai test set
test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    directory=f'{base_dir}/test',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

loss, accuracy = model.evaluate(test_generator)
print(f"Test Accuracy: {accuracy:.2%}")

"""### MODEL 2"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Definisi model CNN
model2 = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(train_generator.num_classes, activation='softmax')
])

# Kompilasi model dengan RMSprop (default learning rate)
model2.compile(
    optimizer=RMSprop(),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Callback
callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)
]

# Training
history = model2.fit(
    train_generator,
    epochs=10,
    validation_data=val_generator,
    callbacks=callbacks
)

"""### MODEL 3"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Model CNN yang dioptimasi
model3 = Sequential([
    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3)),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(64, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(128, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(train_generator.num_classes, activation='softmax')
])

# Kompilasi model
model3.compile(optimizer='adam',
               loss='categorical_crossentropy',
               metrics=['accuracy'])

# Callback
callbacks = [
    EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1),
    ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, verbose=1)
]

# Training model
history3 = model3.fit(
    train_generator,
    epochs=50,
    validation_data=val_generator,
    callbacks=callbacks
)

"""### MODEL 4"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Model CNN lebih dalam (4 Conv2D layers)
model4 = Sequential([
    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3)),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(64, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(128, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(256, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(train_generator.num_classes, activation='softmax')
])

# Kompilasi model
model4.compile(optimizer='adam',
               loss='categorical_crossentropy',
               metrics=['accuracy'])

# Callback
callbacks = [
    EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1),
    ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, verbose=1)
]

# Training model
history4 = model4.fit(
    train_generator,
    epochs=10,
    validation_data=val_generator,
    callbacks=callbacks
)

"""### MODEL Tata"""

import tensorflow as tf
from tensorflow.keras import layers, models

# Contoh input size, sesuaikan kalau berbeda
input_shape = (224, 224, 3)
num_classes = 13

from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom

data_augmentation = tf.keras.Sequential([
    RandomFlip("horizontal"),
    RandomRotation(0.1),
    RandomZoom(0.1),
])

model3 = models.Sequential([
    layers.InputLayer(input_shape=input_shape),
    data_augmentation,

    # Block 1
    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Dropout(0.25),

    # Block 2
    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Dropout(0.25),

    # Block 3
    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Dropout(0.3),

    # Fully Connected
    layers.Flatten(),
    layers.Dense(512, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.5),
    layers.Dense(num_classes, activation='softmax')
])

model3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model3.summary()

from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, verbose=1)

history = model3.fit(
    train_generator,
    validation_data=val_generator,
    epochs=50,
    callbacks=[early_stop, reduce_lr]
)

# Evaluasi pakai test set
test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    directory=f'{base_dir}/test',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

loss, accuracy = model3.evaluate(test_generator)
print(f"Test Accuracy: {accuracy:.2%}")

# package matplotlib
import matplotlib.pyplot as plt

# Plot akurasi
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.legend()
plt.title('Model Accuracy')
plt.show()

import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# --- Parameter Dasar ---
input_shape = (224, 224, 3)
num_classes = 13

# --- Data Augmentation Layer ---
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2),
    layers.RandomContrast(0.2)
], name="data_augmentation")

# --- Fungsi Membangun Model CNN dari Awal (model6) ---
def build_model6_from_scratch(input_shape, num_classes, data_augmentation_layer):
    inputs = layers.Input(shape=input_shape, name="input_layer")

    # Augmentasi data hanya aktif saat training
    x = data_augmentation_layer(inputs)

    # Block 1
    x = layers.Conv2D(32, (3, 3), padding='same', name="conv1a")(x)
    x = layers.BatchNormalization(name="bn1a")(x)
    x = layers.Activation('relu', name="relu1a")(x)
    x = layers.Conv2D(32, (3, 3), padding='same', name="conv1b")(x)
    x = layers.BatchNormalization(name="bn1b")(x)
    x = layers.Activation('relu', name="relu1b")(x)
    x = layers.MaxPooling2D((2, 2), name="pool1")(x)
    x = layers.Dropout(0.25, name="drop1")(x)

    # Block 2
    x = layers.Conv2D(64, (3, 3), padding='same', name="conv2a")(x)
    x = layers.BatchNormalization(name="bn2a")(x)
    x = layers.Activation('relu', name="relu2a")(x)
    x = layers.Conv2D(64, (3, 3), padding='same', name="conv2b")(x)
    x = layers.BatchNormalization(name="bn2b")(x)
    x = layers.Activation('relu', name="relu2b")(x)
    x = layers.MaxPooling2D((2, 2), name="pool2")(x)
    x = layers.Dropout(0.25, name="drop2")(x)

    # Block 3
    x = layers.Conv2D(128, (3, 3), padding='same', name="conv3a")(x)
    x = layers.BatchNormalization(name="bn3a")(x)
    x = layers.Activation('relu', name="relu3a")(x)
    x = layers.Conv2D(128, (3, 3), padding='same', name="conv3b")(x)
    x = layers.BatchNormalization(name="bn3b")(x)
    x = layers.Activation('relu', name="relu3b")(x)
    x = layers.MaxPooling2D((2, 2), name="pool3")(x)
    x = layers.Dropout(0.3, name="drop3")(x)

    # Fully Connected Head
    x = layers.Flatten(name="flatten")(x)
    x = layers.Dense(512, name="dense1")(x)
    x = layers.BatchNormalization(name="bn_dense1")(x)
    x = layers.Activation('relu', name="relu_dense1")(x)
    x = layers.Dropout(0.5, name="drop_dense1")(x)

    outputs = layers.Dense(num_classes, activation='softmax', name="output_layer")(x)

    model = models.Model(inputs, outputs, name="model6_from_scratch")
    return model

model6 = build_model6_from_scratch(input_shape, num_classes, data_augmentation)

# --- Kompilasi Model ---
initial_learning_rate = 1e-3
optimizer = optimizers.Adam(learning_rate=initial_learning_rate)

model6.compile(
    optimizer=optimizer,
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model6.summary()

# --- Callbacks ---
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=15,
    restore_best_weights=True,
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=7,
    min_lr=1e-6,
    verbose=1
)

callbacks_list = [early_stopping, reduce_lr]

# --- Training Model ---
epochs = 30

print("Memulai training model6 (from scratch)...")
history = model6.fit(
    train_generator,
    epochs=epochs,
    validation_data=val_generator,
    callbacks=callbacks_list
)

# Evaluasi pakai test set
test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    directory=f'{base_dir}/test',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

loss, accuracy = model6.evaluate(test_generator)
print(f"Test Accuracy: {accuracy:.2%}")

import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# --- Parameter Dasar ---
input_shape = (224, 224, 3)
num_classes = 13

# --- Data Augmentation Layer ---
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2),
    layers.RandomContrast(0.2)
], name="data_augmentation")

# --- Fungsi Membangun Model CNN dengan Depthwise Separable Conv (model7) ---
def build_model7_depthwise_sep(input_shape, num_classes, data_augmentation_layer):
    inputs = layers.Input(shape=input_shape, name="input_layer")

    # Augmentasi data aktif saat training
    x = data_augmentation_layer(inputs)

    # Block 1
    x = layers.SeparableConv2D(32, (3, 3), padding='same', name="sep_conv1a")(x)
    x = layers.BatchNormalization(name="bn1a")(x)
    x = layers.Activation('relu', name="relu1a")(x)
    x = layers.SeparableConv2D(32, (3, 3), padding='same', name="sep_conv1b")(x)
    x = layers.BatchNormalization(name="bn1b")(x)
    x = layers.Activation('relu', name="relu1b")(x)
    x = layers.MaxPooling2D((2, 2), name="pool1")(x)
    x = layers.Dropout(0.25, name="drop1")(x)

    # Block 2
    x = layers.SeparableConv2D(64, (3, 3), padding='same', name="sep_conv2a")(x)
    x = layers.BatchNormalization(name="bn2a")(x)
    x = layers.Activation('relu', name="relu2a")(x)
    x = layers.SeparableConv2D(64, (3, 3), padding='same', name="sep_conv2b")(x)
    x = layers.BatchNormalization(name="bn2b")(x)
    x = layers.Activation('relu', name="relu2b")(x)
    x = layers.MaxPooling2D((2, 2), name="pool2")(x)
    x = layers.Dropout(0.25, name="drop2")(x)

    # Block 3
    x = layers.SeparableConv2D(128, (3, 3), padding='same', name="sep_conv3a")(x)
    x = layers.BatchNormalization(name="bn3a")(x)
    x = layers.Activation('relu', name="relu3a")(x)
    x = layers.SeparableConv2D(128, (3, 3), padding='same', name="sep_conv3b")(x)
    x = layers.BatchNormalization(name="bn3b")(x)
    x = layers.Activation('relu', name="relu3b")(x)
    x = layers.MaxPooling2D((2, 2), name="pool3")(x)
    x = layers.Dropout(0.3, name="drop3")(x)

    # Block 4
    x = layers.SeparableConv2D(256, (3, 3), padding='same', name="sep_conv4a")(x)
    x = layers.BatchNormalization(name="bn4a")(x)
    x = layers.Activation('relu', name="relu4a")(x)
    x = layers.SeparableConv2D(256, (3, 3), padding='same', name="sep_conv4b")(x)
    x = layers.BatchNormalization(name="bn4b")(x)
    x = layers.Activation('relu', name="relu4b")(x)
    x = layers.MaxPooling2D((2, 2), name="pool4")(x)
    x = layers.Dropout(0.35, name="drop4")(x)

    # Fully Connected Head
    x = layers.Flatten(name="flatten")(x)
    x = layers.Dense(512, name="dense1")(x)
    x = layers.BatchNormalization(name="bn_dense1")(x)
    x = layers.Activation('relu', name="relu_dense1")(x)
    x = layers.Dropout(0.5, name="drop_dense1")(x)

    outputs = layers.Dense(num_classes, activation='softmax', name="output_layer")(x)

    model = models.Model(inputs, outputs, name="model7_depthwise_sep_cnn")
    return model

model7 = build_model7_depthwise_sep(input_shape, num_classes, data_augmentation)

# --- Kompilasi Model ---
initial_learning_rate = 1e-3
optimizer = optimizers.Adam(learning_rate=initial_learning_rate)

model7.compile(
    optimizer=optimizer,
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model7.summary()

# --- Callbacks ---
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=15,
    restore_best_weights=True,
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=7,
    min_lr=1e-6,
    verbose=1
)

callbacks_list = [early_stopping, reduce_lr]

# --- Training Model ---
epochs = 30
print("Memulai training model7 (Depthwise Separable CNN)...")
history = model7.fit(
    train_generator,
    epochs=epochs,
    validation_data=val_generator,
    callbacks=callbacks_list
)

# Evaluasi pakai test set
test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    directory=f'{base_dir}/test',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

loss, accuracy = model7.evaluate(test_generator)
print(f"Test Accuracy: {accuracy:.2%}")

import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2

# --- Parameter Dasar ---
input_shape = (224, 224, 3)
num_classes = 13
L2_LAMBDA = 0.0005

# --- Data Augmentation Layer ---
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2),
    layers.RandomContrast(0.2)
], name="data_augmentation")

# --- Fungsi Membangun Model CNN dengan L2 Regularization dan SpatialDropout2D ---
def build_model8_regularized_cnn(input_shape, num_classes, data_augmentation_layer, l2_lambda):
    inputs = layers.Input(shape=input_shape, name="input_layer")

    x = data_augmentation_layer(inputs)

    # Block 1
    x = layers.Conv2D(32, (3, 3), padding='same', kernel_regularizer=l2(l2_lambda), name="conv1a")(x)
    x = layers.BatchNormalization(name="bn1a")(x)
    x = layers.Activation('relu', name="relu1a")(x)
    x = layers.MaxPooling2D((2, 2), name="pool1")(x)
    x = layers.SpatialDropout2D(0.25, name="spatial_drop1")(x)

    # Block 2
    x = layers.Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(l2_lambda), name="conv2a")(x)
    x = layers.BatchNormalization(name="bn2a")(x)
    x = layers.Activation('relu', name="relu2a")(x)
    x = layers.MaxPooling2D((2, 2), name="pool2")(x)
    x = layers.SpatialDropout2D(0.25, name="spatial_drop2")(x)

    # Block 3
    x = layers.Conv2D(128, (3, 3), padding='same', kernel_regularizer=l2(l2_lambda), name="conv3a")(x)
    x = layers.BatchNormalization(name="bn3a")(x)
    x = layers.Activation('relu', name="relu3a")(x)
    x = layers.MaxPooling2D((2, 2), name="pool3")(x)
    x = layers.SpatialDropout2D(0.3, name="spatial_drop3")(x)

    # Fully Connected Head
    x = layers.Flatten(name="flatten")(x)
    x = layers.Dense(256, kernel_regularizer=l2(l2_lambda), name="dense1")(x)
    x = layers.BatchNormalization(name="bn_dense1")(x)
    x = layers.Activation('relu', name="relu_dense1")(x)
    x = layers.Dropout(0.5, name="drop_dense1")(x)

    outputs = layers.Dense(num_classes, activation='softmax', name="output_layer")(x)

    model = models.Model(inputs, outputs, name="model8_regularized_cnn")
    return model

model8 = build_model8_regularized_cnn(input_shape, num_classes, data_augmentation, L2_LAMBDA)

# --- Kompilasi Model ---
initial_learning_rate = 1e-3
optimizer = optimizers.Adam(learning_rate=initial_learning_rate)

model8.compile(
    optimizer=optimizer,
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model8.summary()

# --- Callbacks ---
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=7,
    restore_best_weights=True,
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=5,
    min_lr=1e-6,
    verbose=1
)

callbacks_list = [early_stopping, reduce_lr]

# --- Training Model ---
epochs = 15
print("Memulai training model8 (Regularized CNN)...")
history = model8.fit(
    train_generator,
    epochs=epochs,
    validation_data=val_generator,
    callbacks=callbacks_list
)

# Evaluasi pakai test set
test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
    directory=f'{base_dir}/test',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

loss, accuracy = model8.evaluate(test_generator)
print(f"Test Accuracy: {accuracy:.2%}")

"""### MODEL 9"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

model9 = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(224,224,3)),
    BatchNormalization(),
    MaxPooling2D(2,2),

    Conv2D(64, (3,3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(2,2),

    Conv2D(128, (3,3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(2,2),

    Conv2D(256, (3,3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(2,2),

    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(train_generator.num_classes, activation='softmax')
])

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.3,
    horizontal_flip=True,
    fill_mode='nearest'
)

train_generator = train_datagen.flow_from_directory(
    directory=f'{base_dir}/train',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)

from tensorflow.keras.optimizers import Adam

optimizer = Adam(learning_rate=1e-4)
model9.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

callbacks = [
    EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)
]

# Training model
history = model9.fit(
    train_generator,
    epochs=30,
    validation_data=val_generator,
    callbacks=callbacks
)

import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras import layers, models, regularizers

# Augmentasi
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.15),
    layers.RandomZoom(0.15),
])

model10 = models.Sequential([
    layers.Input(shape=(224, 224, 3)),
    data_augmentation,

    layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.001)),
    layers.BatchNormalization(),
    layers.MaxPooling2D(),
    layers.Dropout(0.3),

    layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.001)),
    layers.BatchNormalization(),
    layers.MaxPooling2D(),
    layers.Dropout(0.3),

    layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.001)),
    layers.BatchNormalization(),
    layers.MaxPooling2D(),
    layers.Dropout(0.4),

    layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.001)),
    layers.BatchNormalization(),
    layers.MaxPooling2D(),
    layers.Dropout(0.4),

    layers.Flatten(),
    layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    layers.BatchNormalization(),
    layers.Dropout(0.5),

    layers.Dense(13, activation='softmax')
])

model10.compile(optimizer=tf.keras.optimizers.Adam(1e-3),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model10.summary()

from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.3, verbose=1)
checkpoint = ModelCheckpoint("best_model.h5", save_best_only=True, monitor="val_loss")

# Training model
history = model10.fit(
    train_generator,
    validation_data=val_generator,
    epochs=50,
    callbacks=[early_stop, reduce_lr, checkpoint]
)