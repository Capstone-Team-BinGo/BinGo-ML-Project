# -*- coding: utf-8 -*-
"""Modeling_CNN

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JbwAtRobLdxuCRxivup9USpjoOsrJleX
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install gdown
import gdown

file_id = '1M18xmxEKZ-ALyE6DD3e7ih93ZgF99qTG'
output = 'data_final.zip'
gdown.download(f'https://drive.google.com/uc?id={file_id}', output, quiet=False)

import zipfile

with zipfile.ZipFile('data_final.zip', 'r') as zip_ref:
    zip_ref.extractall('data_final')
print("âœ… Data berhasil diekstrak ke folder 'data_final/'")

import tensorflow as tf
import numpy as np
import random
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2

# Paths dataset
train_dir = '/content/data_final/content/data_final_split/train'
val_dir = '/content/data_final/content/data_final_split/val'
test_dir = '/content/data_final/content/data_final_split/test'

# --- Set random seed agar hasil reproducible ---
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
set_seed(42)

# --- Data generators ---
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=30,
    width_shift_range=0.3,
    height_shift_range=0.3,
    shear_range=0.3,
    zoom_range=0.3,
    horizontal_flip=True,
    vertical_flip=False,
    brightness_range=[0.5, 1.5],
    channel_shift_range=20.0,
    fill_mode='nearest'
)

val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

# Cek jumlah kelas dari generator latihan
train_gen = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224,224),
    batch_size=32,
    class_mode='categorical',
    shuffle=True,
    seed=42
)

num_classes = train_gen.num_classes # Mendapatkan jumlah kelas secara dinamis

val_gen = val_datagen.flow_from_directory(
    val_dir,
    target_size=(224,224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False,
    seed=42
)

test_gen = test_datagen.flow_from_directory(
    test_dir,
    target_size=(224,224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False,
    seed=42
)

print(f"Jumlah kelas yang terdeteksi oleh generator: {num_classes}")
print(f"Mapping kelas ke indeks: {train_gen.class_indices}")

"""# MODEL 1"""

# --- Arsitektur Model ---
model = Sequential([
    # Blok 1
    Conv2D(64, (3,3), padding='same', activation='relu', input_shape=(224,224,3), kernel_regularizer=l2(0.0001)),
    BatchNormalization(),
    Conv2D(64, (3,3), padding='same', activation='relu', kernel_regularizer=l2(0.0001)),
    BatchNormalization(),
    MaxPooling2D(2,2),
    Dropout(0.3),

    # Blok 2 - Lebih Banyak Filter
    Conv2D(128, (3,3), padding='same', activation='relu', kernel_regularizer=l2(0.0001)),
    BatchNormalization(),
    Conv2D(128, (3,3), padding='same', activation='relu', kernel_regularizer=l2(0.0001)),
    BatchNormalization(),
    MaxPooling2D(2,2),
    Dropout(0.4),

    # Blok 3 - Lebih Banyak Filter
    Conv2D(256, (3,3), padding='same', activation='relu', kernel_regularizer=l2(0.0001)),
    BatchNormalization(),
    Conv2D(256, (3,3), padding='same', activation='relu', kernel_regularizer=l2(0.0001)),
    BatchNormalization(),
    MaxPooling2D(2,2),
    Dropout(0.5),

    # Blok 4 (Baru - dengan filter lebih besar)
    Conv2D(512, (3,3), padding='same', activation='relu', kernel_regularizer=l2(0.0001)),
    BatchNormalization(),
    Conv2D(512, (3,3), padding='same', activation='relu', kernel_regularizer=l2(0.0001)),
    BatchNormalization(),
    MaxPooling2D(2,2),
    Dropout(0.5),

    Flatten(),
    Dense(512, activation='relu', kernel_regularizer=l2(0.0001)),
    BatchNormalization(),
    Dropout(0.5),
    Dense(num_classes, activation='softmax') # Menggunakan num_classes yang didapat dari generator
])

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])
model.summary()

# --- Callback untuk training ---
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7)

# Training model
history = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=20,
    callbacks=[early_stop, reduce_lr],
    verbose=1
)

"""# MODEL 2"""

# --- Data Augmentation Layer ---
data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2),
    layers.RandomContrast(0.2)
], name="data_augmentation")

# --- Fungsi Membangun Model CNN dari Awal ---
def build_model6_from_scratch(input_shape, num_classes, data_augmentation_layer):
    inputs = layers.Input(shape=input_shape, name="input_layer")
    x = data_augmentation_layer(inputs)

    # Block 1
    x = layers.Conv2D(32, (3, 3), padding='same', name="conv1a")(x)
    x = layers.BatchNormalization(name="bn1a")(x)
    x = layers.Activation('relu', name="relu1a")(x)
    x = layers.Conv2D(32, (3, 3), padding='same', name="conv1b")(x)
    x = layers.BatchNormalization(name="bn1b")(x)
    x = layers.Activation('relu', name="relu1b")(x)
    x = layers.MaxPooling2D((2, 2), name="pool1")(x)
    x = layers.Dropout(0.25, name="drop1")(x)

    # Block 2
    x = layers.Conv2D(64, (3, 3), padding='same', name="conv2a")(x)
    x = layers.BatchNormalization(name="bn2a")(x)
    x = layers.Activation('relu', name="relu2a")(x)
    x = layers.Conv2D(64, (3, 3), padding='same', name="conv2b")(x)
    x = layers.BatchNormalization(name="bn2b")(x)
    x = layers.Activation('relu', name="relu2b")(x)
    x = layers.MaxPooling2D((2, 2), name="pool2")(x)
    x = layers.Dropout(0.25, name="drop2")(x)

    # Block 3
    x = layers.Conv2D(128, (3, 3), padding='same', name="conv3a")(x)
    x = layers.BatchNormalization(name="bn3a")(x)
    x = layers.Activation('relu', name="relu3a")(x)
    x = layers.Conv2D(128, (3, 3), padding='same', name="conv3b")(x)
    x = layers.BatchNormalization(name="bn3b")(x)
    x = layers.Activation('relu', name="relu3b")(x)
    x = layers.MaxPooling2D((2, 2), name="pool3")(x)
    x = layers.Dropout(0.3, name="drop3")(x)

    # Fully Connected Head
    x = layers.Flatten(name="flatten")(x)
    x = layers.Dense(512, name="dense1")(x)
    x = layers.BatchNormalization(name="bn_dense1")(x)
    x = layers.Activation('relu', name="relu_dense1")(x)
    x = layers.Dropout(0.5, name="drop_dense1")(x)

    outputs = layers.Dense(num_classes, activation='softmax', name="output_layer")(x)

    model = models.Model(inputs, outputs, name="model6_from_scratch")
    return model

input_shape = (224, 224, 3)
model6 = build_model6_from_scratch(input_shape, num_classes, data_augmentation)

# --- Kompilasi Model ---
initial_learning_rate = 1e-3
optimizer = optimizers.Adam(learning_rate=initial_learning_rate)

model6.compile(
    optimizer=optimizer,
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model6.summary()

# --- Callbacks ---
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=15,
    restore_best_weights=True,
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=7,
    min_lr=1e-6,
    verbose=1
)

callbacks_list = [early_stopping, reduce_lr]

# --- Training Model ---
epochs = 100

print("Memulai training model6 (from scratch)...")
history = model6.fit(
    train_gen,
    epochs=epochs,
    validation_data=val_gen,
    callbacks=callbacks_list,
    verbose=1
)

# Evaluasi data test
test_loss, test_acc = model6.evaluate(test_gen, steps=test_gen.samples // test_gen.batch_size)
print(f"Akurasi test: {test_acc * 100:.2f}%")

"""# MODEL 3"""

from tensorflow.keras.optimizers import Adam

model = Sequential([
    # Blok 1
    Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(224, 224, 3)),
    BatchNormalization(),
    Conv2D(32, (3,3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2,2)),
    Dropout(0.25),

    # Blok 2
    Conv2D(64, (3,3), activation='relu', padding='same'),
    BatchNormalization(),
    Conv2D(64, (3,3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2,2)),
    Dropout(0.3),

    # Blok 3
    Conv2D(128, (3,3), activation='relu', padding='same'),
    BatchNormalization(),
    Conv2D(128, (3,3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2,2)),
    Dropout(0.4),

    # Fully connected
    Flatten(),
    Dense(256, activation='relu'),
    BatchNormalization(),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

# Compile
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Callbacks
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)
earlystop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1)

# Train model
history = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=25,
    callbacks=[reduce_lr, earlystop]
)

test_loss, test_acc = model.evaluate(test_gen)
print(f"Akurasi pada data test: {test_acc * 100:.2f}%")

"""# MODEL 4"""

# Model CNN sederhana
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(train_gen.num_classes, activation='softmax')
])

# Kompilasi model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Callback
callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)
]

# Training model
history = model.fit(
    train_gen,
    epochs=20,
    validation_data=val_gen,
    callbacks=callbacks
)

loss, accuracy = model.evaluate(test_gen)
print(f"Test Accuracy: {accuracy:.2%}")